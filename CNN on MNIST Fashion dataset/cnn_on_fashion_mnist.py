# -*- coding: utf-8 -*-
"""CNN on Fashion MNIST.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Wbp7IxJXbnEoAflXNMgjwoL8q-NEzeIl
"""

import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as transformer
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime



train_dataset= torchvision.datasets.FashionMNIST(
    root=".",
    train=True,
    transform=transformer.ToTensor(),
    download=True
)

train_dataset.data.max()

train_dataset.data.size()

test_dataset= torchvision.datasets.FashionMNIST(
    root=".",
    train=False,
    transform=transformer.ToTensor()
)

#Total number of classes
k= len(set(train_dataset.targets.numpy()))
print(f"Total number of classes : {k}")

class CNN(nn.Module):
    def __init__(self, k):
        super(CNN, self).__init__()
        self.convLayers = nn.Sequential(
            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=2),
            nn.ReLU(),
            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2),
            nn.ReLU(),
            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2),
            nn.ReLU(),
        )
        self.denseLayers = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(128 * 2 * 2, 512),  # Ensure the size matches the flattened output
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(512, k),
        )

    def forward(self, x):
        out = self.convLayers(x)
        out = out.view(out.size(0), -1)
        out = self.denseLayers(out)
        return out

model=CNN(k)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)
model = CNN(k).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters())

batch_size=128
train_loader =torch.utils.data.DataLoader(
    train_dataset,
    batch_size=batch_size,
    shuffle=True
)

test_loader = torch.utils.data.DataLoader(
    test_dataset,
    batch_size=batch_size,
    shuffle=False
)

def batch_gd(model, criterion, optimizer, train_loader, test_loader, epochs):
    train_losses = np.zeros(epochs)
    test_losses = np.zeros(epochs)
    for it in range(epochs):
        t0 = datetime.now()
        model.train()
        train_loss = []
        for inputs, targets in train_loader:
            inputs = inputs.to(device)
            targets = targets.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
            train_loss.append(loss.item())

        model.eval()
        test_loss = []
        with torch.no_grad():
            for inputs, targets in test_loader:
                inputs = inputs.to(device)
                targets = targets.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, targets)
                test_loss.append(loss.item())

        train_losses[it] = np.mean(train_loss)
        test_losses[it] = np.mean(test_loss)
        dt = datetime.now() - t0
        print(f"Epoch {it+1}/{epochs}, Train Loss: {train_losses[it]:.4f}, Test Loss: {test_losses[it]:.4f}, Duration: {dt}")

    return train_losses, test_losses

train_losses, test_losses= batch_gd(model,criterion,optimizer,train_loader,test_loader,15)

import matplotlib.pyplot as plt

# Plot losses
plt.figure(figsize=(8, 6))  # Optional: Adjust figure size for better visualization
plt.plot(train_losses, label='Train Loss', marker='o')
plt.plot(test_losses, label='Test Loss', marker='s')
plt.xlabel('Epoch', fontsize=12)
plt.ylabel('Loss', fontsize=12)
plt.title('Training and Testing Loss Over Epochs', fontsize=14)
plt.legend(fontsize=12)
plt.grid(True)  # Optional: Add grid for better readability
plt.show()

# Initialize counters for training accuracy
c_total = 0
n_total = 0

model.eval()
with torch.no_grad():
    for inputs, targets in train_loader:
        inputs, targets = inputs.to(device), targets.to(device)

        outputs = model(inputs)

        _, predictions = torch.max(outputs, 1)

        c_total += (predictions == targets).sum().item()
        n_total += targets.size(0)

train_accuracy = c_total / n_total

c_total = 0
n_total = 0

with torch.no_grad():
    for inputs, targets in test_loader:
        inputs, targets = inputs.to(device), targets.to(device)

        outputs = model(inputs)

        _, predictions = torch.max(outputs, 1)

        c_total += (predictions == targets).sum().item()
        n_total += targets.size(0)

test_accuracy = c_total / n_total

# Print accuracies
print(f"Train Accuracy: {train_accuracy * 100:.2f}%")
print(f"Test Accuracy: {test_accuracy * 100:.2f}%")

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Initialize lists to store true and predicted labels
true_labels = []
predicted_labels = []

model.eval()  # Set the model to evaluation mode
with torch.no_grad():
    for inputs, targets in test_loader:
        inputs, targets = inputs.to(device), targets.to(device)

        outputs = model(inputs)
        _, predictions = torch.max(outputs, 1)  # Get the predicted class labels

        # Append the true and predicted labels
        true_labels.extend(targets.cpu().numpy())  # Convert tensors to numpy
        predicted_labels.extend(predictions.cpu().numpy())

# Generate the confusion matrix
cm = confusion_matrix(true_labels, predicted_labels)

# Display the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)

plt.title("Confusion Matrix")
plt.show()

