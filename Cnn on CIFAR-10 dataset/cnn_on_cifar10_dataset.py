# -*- coding: utf-8 -*-
"""CNN on Cifar10 dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12eTUScOZjIehHJGxdIMRa_xEDe6zuAen
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transformer
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime

train_dataset= torchvision.datasets.CIFAR10(
    root=".",
    train=True,
    transform=transformer.ToTensor(),
    download=True
)

train_dataset.data

train_dataset.data.shape

train_dataset.targets

k= len(set(train_dataset.targets))
print("No of Classes: ",k)

test_dataset=torchvision.datasets.CIFAR10(
    train=False,
    root=".",
    transform=transformer.ToTensor()
    )

batch_size =128

train_loader = torch.utils.data.DataLoader(
    shuffle= True,
    batch_size=batch_size,
    dataset=train_dataset
)

test_loader = torch.utils.data.DataLoader(
    shuffle=False,
    dataset=test_dataset,
    batch_size=batch_size
)

class CNN(nn.Module):
    def __init__(self, k):
        super(CNN, self).__init__()

        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=2)
        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2)
        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2)

        self.fc1 = nn.Linear(128 * 3 * 3, 1024)
        self.fc2 = nn.Linear(1024, k)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        x = x.view(-1, 128 * 3 * 3)
        x = F.dropout(x, p=0.5)
        x = F.relu(self.fc1(x))
        x = F.dropout(x, p=0.2)
        x = self.fc2(x)

        return x

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)
model = CNN(k).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters())

def batch_gd(model, criterion, optimizer, train_loader, test_loader, epochs):
    train_losses = np.zeros(epochs)
    test_losses = np.zeros(epochs)
    for it in range(epochs):
        t0 = datetime.now()
        model.train()
        train_loss = []
        for inputs, targets in train_loader:
            inputs = inputs.to(device)
            targets = targets.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
            train_loss.append(loss.item())

        model.eval()
        test_loss = []
        with torch.no_grad():
            for inputs, targets in test_loader:
                inputs = inputs.to(device)
                targets = targets.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, targets)
                test_loss.append(loss.item())

        train_losses[it] = np.mean(train_loss)
        test_losses[it] = np.mean(test_loss)
        dt = datetime.now() - t0
        print(f"Epoch {it+1}/{epochs}, Train Loss: {train_losses[it]:.4f}, Test Loss: {test_losses[it]:.4f}, Duration: {dt}")

    return train_losses, test_losses

train_losses, test_losses= batch_gd(model,criterion,optimizer,train_loader,test_loader,15)

import matplotlib.pyplot as plt

# Plot losses
plt.figure(figsize=(8, 6))  # Optional: Adjust figure size for better visualization
plt.plot(train_losses, label='Train Loss', marker='o')
plt.plot(test_losses, label='Test Loss', marker='s')
plt.xlabel('Epoch', fontsize=12)
plt.ylabel('Loss', fontsize=12)
plt.title('Training and Testing Loss Over Epochs', fontsize=14)
plt.legend(fontsize=12)
plt.grid(True)  # Optional: Add grid for better readability
plt.show()

# Initialize counters for training accuracy
c_total = 0
n_total = 0

model.eval()
with torch.no_grad():
    for inputs, targets in train_loader:
        inputs, targets = inputs.to(device), targets.to(device)

        outputs = model(inputs)

        _, predictions = torch.max(outputs, 1)

        c_total += (predictions == targets).sum().item()
        n_total += targets.size(0)

train_accuracy = c_total / n_total

c_total = 0
n_total = 0

with torch.no_grad():
    for inputs, targets in test_loader:
        inputs, targets = inputs.to(device), targets.to(device)

        outputs = model(inputs)

        _, predictions = torch.max(outputs, 1)

        c_total += (predictions == targets).sum().item()
        n_total += targets.size(0)

test_accuracy = c_total / n_total

# Print accuracies
print(f"Train Accuracy: {train_accuracy * 100:.2f}%")
print(f"Test Accuracy: {test_accuracy * 100:.2f}%")

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Initialize lists to store true and predicted labels
true_labels = []
predicted_labels = []

model.eval()  # Set the model to evaluation mode
with torch.no_grad():
    for inputs, targets in test_loader:
        inputs, targets = inputs.to(device), targets.to(device)

        outputs = model(inputs)
        _, predictions = torch.max(outputs, 1)  # Get the predicted class labels

        # Append the true and predicted labels
        true_labels.extend(targets.cpu().numpy())  # Convert tensors to numpy
        predicted_labels.extend(predictions.cpu().numpy())

# Generate the confusion matrix
cm = confusion_matrix(true_labels, predicted_labels)

# Display the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)

plt.title("Confusion Matrix")
plt.show()

